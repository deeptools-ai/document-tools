{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Document Tools \u00b6 \ud83d\udd27 Tools to automate your document understanding tasks. This package contains tools to automate your document understanding tasks by leveraging the power of \ud83e\udd17 Datasets and \ud83e\udd17 Transformers . With this package, you can (or will be able to): \ud83d\udea7 Create a dataset from a collection of documents. \u2705 Transform a dataset to a format that is suitable for training a model. \ud83d\udea7 Train a model on a dataset. \ud83d\udea7 Evaluate the performance of a model on a dataset of documents. \ud83d\udea7 Export a model to a format that is suitable for inference. Features \u00b6 This project is under development and is in the alpha stage. It is not ready for production use, and if you find any bugs or have any suggestions, please let us know by opening an issue or a pull request . Featured models \u00b6 (https://huggingface.co/docs/transformers/model_doc/dit) x (https://huggingface.co/docs/transformers/model_doc/layoutlmv2) x (https://huggingface.co/docs/transformers/model_doc/layoutlmv3) (https://huggingface.co/docs/transformers/model_doc/layoutxlm) Credits \u00b6 This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Home"},{"location":"#document-tools","text":"\ud83d\udd27 Tools to automate your document understanding tasks. This package contains tools to automate your document understanding tasks by leveraging the power of \ud83e\udd17 Datasets and \ud83e\udd17 Transformers . With this package, you can (or will be able to): \ud83d\udea7 Create a dataset from a collection of documents. \u2705 Transform a dataset to a format that is suitable for training a model. \ud83d\udea7 Train a model on a dataset. \ud83d\udea7 Evaluate the performance of a model on a dataset of documents. \ud83d\udea7 Export a model to a format that is suitable for inference.","title":"Document Tools"},{"location":"#features","text":"This project is under development and is in the alpha stage. It is not ready for production use, and if you find any bugs or have any suggestions, please let us know by opening an issue or a pull request .","title":"Features"},{"location":"#featured-models","text":"(https://huggingface.co/docs/transformers/model_doc/dit) x (https://huggingface.co/docs/transformers/model_doc/layoutlmv2) x (https://huggingface.co/docs/transformers/model_doc/layoutlmv3) (https://huggingface.co/docs/transformers/model_doc/layoutxlm)","title":"Featured models"},{"location":"#credits","text":"This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Credits"},{"location":"api/","text":"Top-level package for Document Tools. LayoutLMv2Encoder \u00b6 Bases: BaseEncoder LayoutLMv2Encoder is the encoder for datasets using LayoutLMv2. Source code in document_tools/encoders/encoders.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class LayoutLMv2Encoder ( BaseEncoder ): \"\"\"LayoutLMv2Encoder is the encoder for datasets using LayoutLMv2.\"\"\" def __init__ ( self , ** kwargs ): \"\"\" Initialize the LayoutLMv2Encoder. Parameters ---------- kwargs : Dict[str, Any] Check the documentation of the LayoutLMv2Processor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutlmv2#transformers.LayoutLMv2Processor \"\"\" super () . __init__ ( ** kwargs ) self . default_model = self . config . get ( \"default_model\" , \"microsoft/layoutlmv2-base-uncased\" ) self . processor = LayoutLMv2Processor . from_pretrained ( self . default_model , ** self . config ) self . features = Features ( { \"image\" : Array3D ( dtype = \"int64\" , shape = ( 3 , 224 , 224 )), \"input_ids\" : Sequence ( feature = Value ( dtype = \"int64\" )), \"attention_mask\" : Sequence ( Value ( dtype = \"int64\" )), \"token_type_ids\" : Sequence ( Value ( dtype = \"int64\" )), \"bbox\" : Array2D ( dtype = \"int64\" , shape = ( 512 , 4 )), \"labels\" : Sequence ( ClassLabel ( num_classes = len ( self . labels ), names = self . labels )), } ) def __call__ ( self , batch : Dict [ str , List ]): \"\"\"Call the LayoutLMv2Encoder.\"\"\" images = [ image . convert ( \"RGB\" ) for image in batch [ \"image\" ]] encoded_inputs = self . processor ( images ) encoded_inputs [ \"labels\" ] = [ label for label in batch [ \"label\" ]] return encoded_inputs __call__ ( batch ) \u00b6 Call the LayoutLMv2Encoder. Source code in document_tools/encoders/encoders.py 83 84 85 86 87 88 def __call__ ( self , batch : Dict [ str , List ]): \"\"\"Call the LayoutLMv2Encoder.\"\"\" images = [ image . convert ( \"RGB\" ) for image in batch [ \"image\" ]] encoded_inputs = self . processor ( images ) encoded_inputs [ \"labels\" ] = [ label for label in batch [ \"label\" ]] return encoded_inputs __init__ ( ** kwargs ) \u00b6 Initialize the LayoutLMv2Encoder. Parameters \u00b6 Dict[str, Any] Check the documentation of the LayoutLMv2Processor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutlmv2#transformers.LayoutLMv2Processor Source code in document_tools/encoders/encoders.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , ** kwargs ): \"\"\" Initialize the LayoutLMv2Encoder. Parameters ---------- kwargs : Dict[str, Any] Check the documentation of the LayoutLMv2Processor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutlmv2#transformers.LayoutLMv2Processor \"\"\" super () . __init__ ( ** kwargs ) self . default_model = self . config . get ( \"default_model\" , \"microsoft/layoutlmv2-base-uncased\" ) self . processor = LayoutLMv2Processor . from_pretrained ( self . default_model , ** self . config ) self . features = Features ( { \"image\" : Array3D ( dtype = \"int64\" , shape = ( 3 , 224 , 224 )), \"input_ids\" : Sequence ( feature = Value ( dtype = \"int64\" )), \"attention_mask\" : Sequence ( Value ( dtype = \"int64\" )), \"token_type_ids\" : Sequence ( Value ( dtype = \"int64\" )), \"bbox\" : Array2D ( dtype = \"int64\" , shape = ( 512 , 4 )), \"labels\" : Sequence ( ClassLabel ( num_classes = len ( self . labels ), names = self . labels )), } ) LayoutLMv3Encoder \u00b6 Bases: BaseEncoder LayoutLMv3Encoder is the encoder for datasets using LayoutLMv3. Source code in document_tools/encoders/encoders.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 class LayoutLMv3Encoder ( BaseEncoder ): \"\"\"LayoutLMv3Encoder is the encoder for datasets using LayoutLMv3.\"\"\" def __init__ ( self , ** kwargs ): \"\"\" Initialize the LayoutLMv3Encoder. Parameters ---------- kwargs : Dict[str, Any] Check the documentation of the LayoutLMv3Processor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutlmv3#transformers.LayoutLMv3Processor \"\"\" super () . __init__ ( ** kwargs ) self . default_model = self . config . get ( \"default_model\" , \"microsoft/layoutlmv3-base\" ) self . processor = LayoutLMv3Processor . from_pretrained ( self . default_model , ** self . config ) self . features = Features ( { \"pixel_values\" : Array3D ( dtype = \"float32\" , shape = ( 3 , 224 , 224 )), \"input_ids\" : Sequence ( feature = Value ( dtype = \"int64\" )), \"attention_mask\" : Sequence ( Value ( dtype = \"int64\" )), \"bbox\" : Array2D ( dtype = \"int64\" , shape = ( 512 , 4 )), \"labels\" : Sequence ( feature = Value ( dtype = 'int64' )), } ) def __call__ ( self , batch : Dict [ str , List ]): \"\"\"Call the LayoutLMv3Encoder.\"\"\" images = [ image . convert ( \"RGB\" ) for image in batch [ \"image\" ]] encoded_inputs = self . processor ( images ) encoded_inputs [ \"labels\" ] = [ label for label in batch [ \"label\" ]] return encoded_inputs __call__ ( batch ) \u00b6 Call the LayoutLMv3Encoder. Source code in document_tools/encoders/encoders.py 117 118 119 120 121 122 def __call__ ( self , batch : Dict [ str , List ]): \"\"\"Call the LayoutLMv3Encoder.\"\"\" images = [ image . convert ( \"RGB\" ) for image in batch [ \"image\" ]] encoded_inputs = self . processor ( images ) encoded_inputs [ \"labels\" ] = [ label for label in batch [ \"label\" ]] return encoded_inputs __init__ ( ** kwargs ) \u00b6 Initialize the LayoutLMv3Encoder. Parameters \u00b6 Dict[str, Any] Check the documentation of the LayoutLMv3Processor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutlmv3#transformers.LayoutLMv3Processor Source code in document_tools/encoders/encoders.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def __init__ ( self , ** kwargs ): \"\"\" Initialize the LayoutLMv3Encoder. Parameters ---------- kwargs : Dict[str, Any] Check the documentation of the LayoutLMv3Processor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutlmv3#transformers.LayoutLMv3Processor \"\"\" super () . __init__ ( ** kwargs ) self . default_model = self . config . get ( \"default_model\" , \"microsoft/layoutlmv3-base\" ) self . processor = LayoutLMv3Processor . from_pretrained ( self . default_model , ** self . config ) self . features = Features ( { \"pixel_values\" : Array3D ( dtype = \"float32\" , shape = ( 3 , 224 , 224 )), \"input_ids\" : Sequence ( feature = Value ( dtype = \"int64\" )), \"attention_mask\" : Sequence ( Value ( dtype = \"int64\" )), \"bbox\" : Array2D ( dtype = \"int64\" , shape = ( 512 , 4 )), \"labels\" : Sequence ( feature = Value ( dtype = 'int64' )), } ) LayoutXLMEncoder \u00b6 Bases: BaseEncoder LayoutXLMEncoder is the encoder for datasets using LayoutXLM. Source code in document_tools/encoders/encoders.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 class LayoutXLMEncoder ( BaseEncoder ): \"\"\"LayoutXLMEncoder is the encoder for datasets using LayoutXLM.\"\"\" def __init__ ( self , ** kwargs ): \"\"\" Initialize the LayoutXLMEncoder. Parameters ---------- kwargs : Dict[str, Any] Check the documentation of the LayoutXLMProcessor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutxlm#transformers.LayoutXLMProcessor \"\"\" super () . __init__ ( ** kwargs ) self . default_model = self . config . get ( \"default_model\" , \"microsoft/layoutxlm-base\" ) self . processor = LayoutXLMProcessor . from_pretrained ( self . default_model , ** self . config ) self . features = Features ( { \"image\" : Array3D ( dtype = \"int64\" , shape = ( 3 , 224 , 224 )), \"input_ids\" : Sequence ( feature = Value ( dtype = \"int64\" )), \"attention_mask\" : Sequence ( Value ( dtype = \"int64\" )), \"token_type_ids\" : Sequence ( Value ( dtype = \"int64\" )), \"bbox\" : Array2D ( dtype = \"int64\" , shape = ( 512 , 4 )), \"labels\" : ClassLabel ( num_classes = len ( self . labels ), names = self . labels ), } ) def __call__ ( self , batch : Dict [ str , List ]): \"\"\"Call the LayoutXLMEncoder.\"\"\" images = [ image . convert ( \"RGB\" ) for image in batch [ \"image\" ]] encoded_inputs = self . processor ( images ) encoded_inputs [ \"labels\" ] = [ label for label in batch [ \"label\" ]] return encoded_inputs __call__ ( batch ) \u00b6 Call the LayoutXLMEncoder. Source code in document_tools/encoders/encoders.py 152 153 154 155 156 157 def __call__ ( self , batch : Dict [ str , List ]): \"\"\"Call the LayoutXLMEncoder.\"\"\" images = [ image . convert ( \"RGB\" ) for image in batch [ \"image\" ]] encoded_inputs = self . processor ( images ) encoded_inputs [ \"labels\" ] = [ label for label in batch [ \"label\" ]] return encoded_inputs __init__ ( ** kwargs ) \u00b6 Initialize the LayoutXLMEncoder. Parameters \u00b6 Dict[str, Any] Check the documentation of the LayoutXLMProcessor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutxlm#transformers.LayoutXLMProcessor Source code in document_tools/encoders/encoders.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def __init__ ( self , ** kwargs ): \"\"\" Initialize the LayoutXLMEncoder. Parameters ---------- kwargs : Dict[str, Any] Check the documentation of the LayoutXLMProcessor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutxlm#transformers.LayoutXLMProcessor \"\"\" super () . __init__ ( ** kwargs ) self . default_model = self . config . get ( \"default_model\" , \"microsoft/layoutxlm-base\" ) self . processor = LayoutXLMProcessor . from_pretrained ( self . default_model , ** self . config ) self . features = Features ( { \"image\" : Array3D ( dtype = \"int64\" , shape = ( 3 , 224 , 224 )), \"input_ids\" : Sequence ( feature = Value ( dtype = \"int64\" )), \"attention_mask\" : Sequence ( Value ( dtype = \"int64\" )), \"token_type_ids\" : Sequence ( Value ( dtype = \"int64\" )), \"bbox\" : Array2D ( dtype = \"int64\" , shape = ( 512 , 4 )), \"labels\" : ClassLabel ( num_classes = len ( self . labels ), names = self . labels ), } ) tokenize_dataset ( dataset , target_model = None , image_column = 'image' , label_column = 'label' , batched = True , batch_size = 2 , cache_file_names = None , keep_in_memory = False , num_proc = None , processor_config = None , save_to_disk = False , save_path = None ) \u00b6 Tokenize a dataset using a target model and return a new dataset with the encoded features and labels. Parameters \u00b6 Dataset or DatasetDict, required Dataset to be tokenized. str, optional (default=None) Target model to use for tokenization. str (default=\"image\") Name of the column containing the image. str (default=\"label\") Name of the column containing the label. bool (default=True) Whether to use batched encoding. int, optional (default=2) Batch size for batched encoding. Dict[str, Optional[str]], optional (default=None) Dictionary containing the cache file names for each target model. bool (default=False) Whether to keep the dataset in memory. int, optional (default=None) Number of processes to use for batched encoding. Dict[str, Any], optional (default=None) Configuration for the processor of the target model. bool (default=False) Whether to save the dataset to disk or not. str (default=None) Path to save the dataset to disk if save_to_disk is True. Returns \u00b6 DatasetDict Dataset with the encoded features and labels. Raises \u00b6 ValueError If there is no target model for the dataset. Or if saving to disk is requested but the save path is not provided. KeyError If the target model is not supported. TypeError If the dataset is not a Dataset or DatasetDict. Source code in document_tools/tokenize.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def tokenize_dataset ( dataset : Union [ Dataset , DatasetDict ], target_model : str = None , image_column : str = \"image\" , label_column : str = \"label\" , batched : bool = True , batch_size : Optional [ int ] = 2 , cache_file_names : Optional [ Dict [ str , Optional [ str ]]] = None , keep_in_memory : bool = False , num_proc : Optional [ int ] = None , processor_config : Optional [ Dict [ str , Any ]] = None , save_to_disk : bool = False , save_path : str = None , ) -> DatasetDict : \"\"\" Tokenize a dataset using a target model and return a new dataset with the encoded features and labels. Parameters ---------- dataset : Dataset or DatasetDict, required Dataset to be tokenized. target_model : str, optional (default=None) Target model to use for tokenization. image_column : str (default=\"image\") Name of the column containing the image. label_column : str (default=\"label\") Name of the column containing the label. batched : bool (default=True) Whether to use batched encoding. batch_size : int, optional (default=2) Batch size for batched encoding. cache_file_names : Dict[str, Optional[str]], optional (default=None) Dictionary containing the cache file names for each target model. keep_in_memory : bool (default=False) Whether to keep the dataset in memory. num_proc : int, optional (default=None) Number of processes to use for batched encoding. processor_config : Dict[str, Any], optional (default=None) Configuration for the processor of the target model. save_to_disk : bool (default=False) Whether to save the dataset to disk or not. save_path : str (default=None) Path to save the dataset to disk if `save_to_disk` is True. Returns ------- DatasetDict Dataset with the encoded features and labels. Raises ------ ValueError If there is no target model for the dataset. Or if saving to disk is requested but the save path is not provided. KeyError If the target model is not supported. TypeError If the dataset is not a Dataset or DatasetDict. \"\"\" if not target_model : raise ValueError ( \"\"\"You need to specify the target architecture you want to use to tokenize your dataset.\"\"\" ) else : try : TARGET_MODELS [ target_model ] except KeyError : raise KeyError ( f \"\"\" You specified a `target_model` that is not supported. Available models: { list ( TARGET_MODELS . keys ()) } If you think that new model should be available, please feel free to open a new issue on the project repository: https://github.com/deeptools-ai/document-tools/issues \"\"\" ) if save_to_disk and save_path is None : raise ValueError ( \"\"\" You need to specify a path to save the dataset, because you chose to save it to disk. You can disable saving to disk by setting `save_to_disk=False`. \"\"\" ) elif not save_to_disk and save_path is not None : logger . warning ( \"\"\" You have indicated a path to save the dataset, but have chosen not to save it to disk. You need to add `save_to_disk=True` to the call to `tokenize_dataset` to save the dataset to disk. \"\"\" ) else : logger . info ( \"\"\" The dataset will not be saved to disk. If you want to save it to disk, add `save_to_disk=True` to the call to `tokenize_dataset`. \"\"\" ) dataset = copy . deepcopy ( dataset ) if isinstance ( dataset , DatasetDict ): tmp_dataset = dataset dataset_first_key = list ( tmp_dataset . keys ())[ 0 ] elif isinstance ( dataset , Dataset ): tmp_dataset = DatasetDict () dataset_first_key = \"train\" tmp_dataset [ dataset_first_key ] = dataset else : raise TypeError ( f \"The dataset has to be either a `Dataset` or a `DatasetDict`. You provided: { type ( dataset ) } \" ) if isinstance ( tmp_dataset [ dataset_first_key ] . features [ label_column ] . feature , ClassLabel ): labels = tmp_dataset [ dataset_first_key ] . features [ label_column ] . feature . names else : labels = _get_label_list ( tmp_dataset [ dataset_first_key ][ label_column ]) encoder = TARGET_MODELS [ target_model ]( config = processor_config , labels = labels ) features = encoder . features encoded_dataset = tmp_dataset . map ( encoder , features = features , remove_columns = [ image_column , label_column ], batched = batched , batch_size = batch_size , cache_file_names = cache_file_names , keep_in_memory = keep_in_memory , num_proc = num_proc , ) if save_to_disk : try : encoded_dataset . save_to_disk ( save_path ) # type: ignore except Exception as e : logger . error ( e ) return encoded_dataset","title":"Modules"},{"location":"api/#document_tools.LayoutLMv2Encoder","text":"Bases: BaseEncoder LayoutLMv2Encoder is the encoder for datasets using LayoutLMv2. Source code in document_tools/encoders/encoders.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class LayoutLMv2Encoder ( BaseEncoder ): \"\"\"LayoutLMv2Encoder is the encoder for datasets using LayoutLMv2.\"\"\" def __init__ ( self , ** kwargs ): \"\"\" Initialize the LayoutLMv2Encoder. Parameters ---------- kwargs : Dict[str, Any] Check the documentation of the LayoutLMv2Processor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutlmv2#transformers.LayoutLMv2Processor \"\"\" super () . __init__ ( ** kwargs ) self . default_model = self . config . get ( \"default_model\" , \"microsoft/layoutlmv2-base-uncased\" ) self . processor = LayoutLMv2Processor . from_pretrained ( self . default_model , ** self . config ) self . features = Features ( { \"image\" : Array3D ( dtype = \"int64\" , shape = ( 3 , 224 , 224 )), \"input_ids\" : Sequence ( feature = Value ( dtype = \"int64\" )), \"attention_mask\" : Sequence ( Value ( dtype = \"int64\" )), \"token_type_ids\" : Sequence ( Value ( dtype = \"int64\" )), \"bbox\" : Array2D ( dtype = \"int64\" , shape = ( 512 , 4 )), \"labels\" : Sequence ( ClassLabel ( num_classes = len ( self . labels ), names = self . labels )), } ) def __call__ ( self , batch : Dict [ str , List ]): \"\"\"Call the LayoutLMv2Encoder.\"\"\" images = [ image . convert ( \"RGB\" ) for image in batch [ \"image\" ]] encoded_inputs = self . processor ( images ) encoded_inputs [ \"labels\" ] = [ label for label in batch [ \"label\" ]] return encoded_inputs","title":"LayoutLMv2Encoder"},{"location":"api/#document_tools.encoders.encoders.LayoutLMv2Encoder.__call__","text":"Call the LayoutLMv2Encoder. Source code in document_tools/encoders/encoders.py 83 84 85 86 87 88 def __call__ ( self , batch : Dict [ str , List ]): \"\"\"Call the LayoutLMv2Encoder.\"\"\" images = [ image . convert ( \"RGB\" ) for image in batch [ \"image\" ]] encoded_inputs = self . processor ( images ) encoded_inputs [ \"labels\" ] = [ label for label in batch [ \"label\" ]] return encoded_inputs","title":"__call__()"},{"location":"api/#document_tools.encoders.encoders.LayoutLMv2Encoder.__init__","text":"Initialize the LayoutLMv2Encoder.","title":"__init__()"},{"location":"api/#document_tools.encoders.encoders.LayoutLMv2Encoder.__init__--parameters","text":"Dict[str, Any] Check the documentation of the LayoutLMv2Processor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutlmv2#transformers.LayoutLMv2Processor Source code in document_tools/encoders/encoders.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , ** kwargs ): \"\"\" Initialize the LayoutLMv2Encoder. Parameters ---------- kwargs : Dict[str, Any] Check the documentation of the LayoutLMv2Processor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutlmv2#transformers.LayoutLMv2Processor \"\"\" super () . __init__ ( ** kwargs ) self . default_model = self . config . get ( \"default_model\" , \"microsoft/layoutlmv2-base-uncased\" ) self . processor = LayoutLMv2Processor . from_pretrained ( self . default_model , ** self . config ) self . features = Features ( { \"image\" : Array3D ( dtype = \"int64\" , shape = ( 3 , 224 , 224 )), \"input_ids\" : Sequence ( feature = Value ( dtype = \"int64\" )), \"attention_mask\" : Sequence ( Value ( dtype = \"int64\" )), \"token_type_ids\" : Sequence ( Value ( dtype = \"int64\" )), \"bbox\" : Array2D ( dtype = \"int64\" , shape = ( 512 , 4 )), \"labels\" : Sequence ( ClassLabel ( num_classes = len ( self . labels ), names = self . labels )), } )","title":"Parameters"},{"location":"api/#document_tools.LayoutLMv3Encoder","text":"Bases: BaseEncoder LayoutLMv3Encoder is the encoder for datasets using LayoutLMv3. Source code in document_tools/encoders/encoders.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 class LayoutLMv3Encoder ( BaseEncoder ): \"\"\"LayoutLMv3Encoder is the encoder for datasets using LayoutLMv3.\"\"\" def __init__ ( self , ** kwargs ): \"\"\" Initialize the LayoutLMv3Encoder. Parameters ---------- kwargs : Dict[str, Any] Check the documentation of the LayoutLMv3Processor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutlmv3#transformers.LayoutLMv3Processor \"\"\" super () . __init__ ( ** kwargs ) self . default_model = self . config . get ( \"default_model\" , \"microsoft/layoutlmv3-base\" ) self . processor = LayoutLMv3Processor . from_pretrained ( self . default_model , ** self . config ) self . features = Features ( { \"pixel_values\" : Array3D ( dtype = \"float32\" , shape = ( 3 , 224 , 224 )), \"input_ids\" : Sequence ( feature = Value ( dtype = \"int64\" )), \"attention_mask\" : Sequence ( Value ( dtype = \"int64\" )), \"bbox\" : Array2D ( dtype = \"int64\" , shape = ( 512 , 4 )), \"labels\" : Sequence ( feature = Value ( dtype = 'int64' )), } ) def __call__ ( self , batch : Dict [ str , List ]): \"\"\"Call the LayoutLMv3Encoder.\"\"\" images = [ image . convert ( \"RGB\" ) for image in batch [ \"image\" ]] encoded_inputs = self . processor ( images ) encoded_inputs [ \"labels\" ] = [ label for label in batch [ \"label\" ]] return encoded_inputs","title":"LayoutLMv3Encoder"},{"location":"api/#document_tools.encoders.encoders.LayoutLMv3Encoder.__call__","text":"Call the LayoutLMv3Encoder. Source code in document_tools/encoders/encoders.py 117 118 119 120 121 122 def __call__ ( self , batch : Dict [ str , List ]): \"\"\"Call the LayoutLMv3Encoder.\"\"\" images = [ image . convert ( \"RGB\" ) for image in batch [ \"image\" ]] encoded_inputs = self . processor ( images ) encoded_inputs [ \"labels\" ] = [ label for label in batch [ \"label\" ]] return encoded_inputs","title":"__call__()"},{"location":"api/#document_tools.encoders.encoders.LayoutLMv3Encoder.__init__","text":"Initialize the LayoutLMv3Encoder.","title":"__init__()"},{"location":"api/#document_tools.encoders.encoders.LayoutLMv3Encoder.__init__--parameters","text":"Dict[str, Any] Check the documentation of the LayoutLMv3Processor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutlmv3#transformers.LayoutLMv3Processor Source code in document_tools/encoders/encoders.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def __init__ ( self , ** kwargs ): \"\"\" Initialize the LayoutLMv3Encoder. Parameters ---------- kwargs : Dict[str, Any] Check the documentation of the LayoutLMv3Processor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutlmv3#transformers.LayoutLMv3Processor \"\"\" super () . __init__ ( ** kwargs ) self . default_model = self . config . get ( \"default_model\" , \"microsoft/layoutlmv3-base\" ) self . processor = LayoutLMv3Processor . from_pretrained ( self . default_model , ** self . config ) self . features = Features ( { \"pixel_values\" : Array3D ( dtype = \"float32\" , shape = ( 3 , 224 , 224 )), \"input_ids\" : Sequence ( feature = Value ( dtype = \"int64\" )), \"attention_mask\" : Sequence ( Value ( dtype = \"int64\" )), \"bbox\" : Array2D ( dtype = \"int64\" , shape = ( 512 , 4 )), \"labels\" : Sequence ( feature = Value ( dtype = 'int64' )), } )","title":"Parameters"},{"location":"api/#document_tools.LayoutXLMEncoder","text":"Bases: BaseEncoder LayoutXLMEncoder is the encoder for datasets using LayoutXLM. Source code in document_tools/encoders/encoders.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 class LayoutXLMEncoder ( BaseEncoder ): \"\"\"LayoutXLMEncoder is the encoder for datasets using LayoutXLM.\"\"\" def __init__ ( self , ** kwargs ): \"\"\" Initialize the LayoutXLMEncoder. Parameters ---------- kwargs : Dict[str, Any] Check the documentation of the LayoutXLMProcessor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutxlm#transformers.LayoutXLMProcessor \"\"\" super () . __init__ ( ** kwargs ) self . default_model = self . config . get ( \"default_model\" , \"microsoft/layoutxlm-base\" ) self . processor = LayoutXLMProcessor . from_pretrained ( self . default_model , ** self . config ) self . features = Features ( { \"image\" : Array3D ( dtype = \"int64\" , shape = ( 3 , 224 , 224 )), \"input_ids\" : Sequence ( feature = Value ( dtype = \"int64\" )), \"attention_mask\" : Sequence ( Value ( dtype = \"int64\" )), \"token_type_ids\" : Sequence ( Value ( dtype = \"int64\" )), \"bbox\" : Array2D ( dtype = \"int64\" , shape = ( 512 , 4 )), \"labels\" : ClassLabel ( num_classes = len ( self . labels ), names = self . labels ), } ) def __call__ ( self , batch : Dict [ str , List ]): \"\"\"Call the LayoutXLMEncoder.\"\"\" images = [ image . convert ( \"RGB\" ) for image in batch [ \"image\" ]] encoded_inputs = self . processor ( images ) encoded_inputs [ \"labels\" ] = [ label for label in batch [ \"label\" ]] return encoded_inputs","title":"LayoutXLMEncoder"},{"location":"api/#document_tools.encoders.encoders.LayoutXLMEncoder.__call__","text":"Call the LayoutXLMEncoder. Source code in document_tools/encoders/encoders.py 152 153 154 155 156 157 def __call__ ( self , batch : Dict [ str , List ]): \"\"\"Call the LayoutXLMEncoder.\"\"\" images = [ image . convert ( \"RGB\" ) for image in batch [ \"image\" ]] encoded_inputs = self . processor ( images ) encoded_inputs [ \"labels\" ] = [ label for label in batch [ \"label\" ]] return encoded_inputs","title":"__call__()"},{"location":"api/#document_tools.encoders.encoders.LayoutXLMEncoder.__init__","text":"Initialize the LayoutXLMEncoder.","title":"__init__()"},{"location":"api/#document_tools.encoders.encoders.LayoutXLMEncoder.__init__--parameters","text":"Dict[str, Any] Check the documentation of the LayoutXLMProcessor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutxlm#transformers.LayoutXLMProcessor Source code in document_tools/encoders/encoders.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def __init__ ( self , ** kwargs ): \"\"\" Initialize the LayoutXLMEncoder. Parameters ---------- kwargs : Dict[str, Any] Check the documentation of the LayoutXLMProcessor for the available parameters : https://huggingface.co/docs/transformers/model_doc/layoutxlm#transformers.LayoutXLMProcessor \"\"\" super () . __init__ ( ** kwargs ) self . default_model = self . config . get ( \"default_model\" , \"microsoft/layoutxlm-base\" ) self . processor = LayoutXLMProcessor . from_pretrained ( self . default_model , ** self . config ) self . features = Features ( { \"image\" : Array3D ( dtype = \"int64\" , shape = ( 3 , 224 , 224 )), \"input_ids\" : Sequence ( feature = Value ( dtype = \"int64\" )), \"attention_mask\" : Sequence ( Value ( dtype = \"int64\" )), \"token_type_ids\" : Sequence ( Value ( dtype = \"int64\" )), \"bbox\" : Array2D ( dtype = \"int64\" , shape = ( 512 , 4 )), \"labels\" : ClassLabel ( num_classes = len ( self . labels ), names = self . labels ), } )","title":"Parameters"},{"location":"api/#document_tools.tokenize_dataset","text":"Tokenize a dataset using a target model and return a new dataset with the encoded features and labels.","title":"tokenize_dataset()"},{"location":"api/#document_tools.tokenize_dataset--parameters","text":"Dataset or DatasetDict, required Dataset to be tokenized. str, optional (default=None) Target model to use for tokenization. str (default=\"image\") Name of the column containing the image. str (default=\"label\") Name of the column containing the label. bool (default=True) Whether to use batched encoding. int, optional (default=2) Batch size for batched encoding. Dict[str, Optional[str]], optional (default=None) Dictionary containing the cache file names for each target model. bool (default=False) Whether to keep the dataset in memory. int, optional (default=None) Number of processes to use for batched encoding. Dict[str, Any], optional (default=None) Configuration for the processor of the target model. bool (default=False) Whether to save the dataset to disk or not. str (default=None) Path to save the dataset to disk if save_to_disk is True.","title":"Parameters"},{"location":"api/#document_tools.tokenize_dataset--returns","text":"DatasetDict Dataset with the encoded features and labels.","title":"Returns"},{"location":"api/#document_tools.tokenize_dataset--raises","text":"ValueError If there is no target model for the dataset. Or if saving to disk is requested but the save path is not provided. KeyError If the target model is not supported. TypeError If the dataset is not a Dataset or DatasetDict. Source code in document_tools/tokenize.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def tokenize_dataset ( dataset : Union [ Dataset , DatasetDict ], target_model : str = None , image_column : str = \"image\" , label_column : str = \"label\" , batched : bool = True , batch_size : Optional [ int ] = 2 , cache_file_names : Optional [ Dict [ str , Optional [ str ]]] = None , keep_in_memory : bool = False , num_proc : Optional [ int ] = None , processor_config : Optional [ Dict [ str , Any ]] = None , save_to_disk : bool = False , save_path : str = None , ) -> DatasetDict : \"\"\" Tokenize a dataset using a target model and return a new dataset with the encoded features and labels. Parameters ---------- dataset : Dataset or DatasetDict, required Dataset to be tokenized. target_model : str, optional (default=None) Target model to use for tokenization. image_column : str (default=\"image\") Name of the column containing the image. label_column : str (default=\"label\") Name of the column containing the label. batched : bool (default=True) Whether to use batched encoding. batch_size : int, optional (default=2) Batch size for batched encoding. cache_file_names : Dict[str, Optional[str]], optional (default=None) Dictionary containing the cache file names for each target model. keep_in_memory : bool (default=False) Whether to keep the dataset in memory. num_proc : int, optional (default=None) Number of processes to use for batched encoding. processor_config : Dict[str, Any], optional (default=None) Configuration for the processor of the target model. save_to_disk : bool (default=False) Whether to save the dataset to disk or not. save_path : str (default=None) Path to save the dataset to disk if `save_to_disk` is True. Returns ------- DatasetDict Dataset with the encoded features and labels. Raises ------ ValueError If there is no target model for the dataset. Or if saving to disk is requested but the save path is not provided. KeyError If the target model is not supported. TypeError If the dataset is not a Dataset or DatasetDict. \"\"\" if not target_model : raise ValueError ( \"\"\"You need to specify the target architecture you want to use to tokenize your dataset.\"\"\" ) else : try : TARGET_MODELS [ target_model ] except KeyError : raise KeyError ( f \"\"\" You specified a `target_model` that is not supported. Available models: { list ( TARGET_MODELS . keys ()) } If you think that new model should be available, please feel free to open a new issue on the project repository: https://github.com/deeptools-ai/document-tools/issues \"\"\" ) if save_to_disk and save_path is None : raise ValueError ( \"\"\" You need to specify a path to save the dataset, because you chose to save it to disk. You can disable saving to disk by setting `save_to_disk=False`. \"\"\" ) elif not save_to_disk and save_path is not None : logger . warning ( \"\"\" You have indicated a path to save the dataset, but have chosen not to save it to disk. You need to add `save_to_disk=True` to the call to `tokenize_dataset` to save the dataset to disk. \"\"\" ) else : logger . info ( \"\"\" The dataset will not be saved to disk. If you want to save it to disk, add `save_to_disk=True` to the call to `tokenize_dataset`. \"\"\" ) dataset = copy . deepcopy ( dataset ) if isinstance ( dataset , DatasetDict ): tmp_dataset = dataset dataset_first_key = list ( tmp_dataset . keys ())[ 0 ] elif isinstance ( dataset , Dataset ): tmp_dataset = DatasetDict () dataset_first_key = \"train\" tmp_dataset [ dataset_first_key ] = dataset else : raise TypeError ( f \"The dataset has to be either a `Dataset` or a `DatasetDict`. You provided: { type ( dataset ) } \" ) if isinstance ( tmp_dataset [ dataset_first_key ] . features [ label_column ] . feature , ClassLabel ): labels = tmp_dataset [ dataset_first_key ] . features [ label_column ] . feature . names else : labels = _get_label_list ( tmp_dataset [ dataset_first_key ][ label_column ]) encoder = TARGET_MODELS [ target_model ]( config = processor_config , labels = labels ) features = encoder . features encoded_dataset = tmp_dataset . map ( encoder , features = features , remove_columns = [ image_column , label_column ], batched = batched , batch_size = batch_size , cache_file_names = cache_file_names , keep_in_memory = keep_in_memory , num_proc = num_proc , ) if save_to_disk : try : encoded_dataset . save_to_disk ( save_path ) # type: ignore except Exception as e : logger . error ( e ) return encoded_dataset","title":"Raises"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [Unreleased] \u00b6 [0.1.0] - 2022-06-24 \u00b6 Added \u00b6 First release of the package on PyPI. \ud83c\udf89 Added repository and dependencies. Added documentation via GitHub pages. Added automatic dataset tokenization for LayoutLMv2 and LayoutLMv3 .","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"changelog/#010---2022-06-24","text":"","title":"[0.1.0] - 2022-06-24"},{"location":"changelog/#added","text":"First release of the package on PyPI. \ud83c\udf89 Added repository and dependencies. Added documentation via GitHub pages. Added automatic dataset tokenization for LayoutLMv2 and LayoutLMv3 .","title":"Added"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/deeptools-ai/document-tools/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 Document Tools could always use more documentation, whether as part of the official Document Tools docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/deeptools-ai/document-tools/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up document-tools for local development. Fork the document-tools repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/document-tools.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8 and 3.9. Check https://github.com/deeptools-ai/document-tools/actions and make sure that the tests pass for all supported Python versions. Tips \u00b6 $ poetry run pytest tests/test_document_tools.py To run a subset of tests. Deploying \u00b6 A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/deeptools-ai/document-tools/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"Document Tools could always use more documentation, whether as part of the official Document Tools docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/deeptools-ai/document-tools/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up document-tools for local development. Fork the document-tools repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/document-tools.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.6, 3.7, 3.8 and 3.9. Check https://github.com/deeptools-ai/document-tools/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#tips","text":"$ poetry run pytest tests/test_document_tools.py To run a subset of tests.","title":"Tips"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install Document Tools, run this command in your terminal: $ pip install document-tools This is the preferred method to install \ud83d\udd27 Document Tools, as it will always install the most recent stable release. From source \u00b6 The source for Document Tools can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/deeptools-ai/document-tools Or download the tarball : $ curl -OJL https://github.com/deeptools-ai/document-tools/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install Document Tools, run this command in your terminal: $ pip install document-tools This is the preferred method to install \ud83d\udd27 Document Tools, as it will always install the most recent stable release.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for Document Tools can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/deeptools-ai/document-tools Or download the tarball : $ curl -OJL https://github.com/deeptools-ai/document-tools/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"From source"},{"location":"usage/","text":"Usage \u00b6 To use Document Tools in a project from datasets import load_dataset from document_tools import tokenize_dataset # Load a dataset from \ud83e\udd17 Hub dataset = load_dataset ( \"deeptools-ai/test-document-invoice\" , split = \"train\" ) # Tokenize the dataset tokenized_dataset = tokenize_dataset ( dataset , target_model = \"layoutlmv3\" ) You can also save the tokenized dataset to \ud83e\udd17 Hub: tokenized_dataset . push_to_hub ( \"YourName/YourProject\" ) Or save it directly to your local machine as a \ud83e\udd17 Dataset: tokenized_dataset = tokenize_dataset ( dataset , target_model = \"layoutlmv3\" , save_to_disk = True , save_path = \"path/to/save/to\" ) Learn more about the available parameters for tokenize_dataset in the documentation","title":"Usage"},{"location":"usage/#usage","text":"To use Document Tools in a project from datasets import load_dataset from document_tools import tokenize_dataset # Load a dataset from \ud83e\udd17 Hub dataset = load_dataset ( \"deeptools-ai/test-document-invoice\" , split = \"train\" ) # Tokenize the dataset tokenized_dataset = tokenize_dataset ( dataset , target_model = \"layoutlmv3\" ) You can also save the tokenized dataset to \ud83e\udd17 Hub: tokenized_dataset . push_to_hub ( \"YourName/YourProject\" ) Or save it directly to your local machine as a \ud83e\udd17 Dataset: tokenized_dataset = tokenize_dataset ( dataset , target_model = \"layoutlmv3\" , save_to_disk = True , save_path = \"path/to/save/to\" ) Learn more about the available parameters for tokenize_dataset in the documentation","title":"Usage"}]}